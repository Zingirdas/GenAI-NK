{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the OpenAI API key\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Print the OpenAI API key\n",
    "print(openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from rich import print\n",
    "\n",
    "def generate_embedding(text, model=\"text-embedding-ada-002\", api_key=openai_key):\n",
    "    \"\"\"Generate embeddings for a given text using OpenAI's API.\"\"\"\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    response = client.embeddings.create(input=text, model=model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "print(generate_embedding(\"Hello, world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Split blog post into <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">66</span> sub-documents.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Split blog post into \u001b[1;36m66\u001b[0m sub-documents.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jauni\\generative-ai-python-2025-02\\.venv\\Lib\\site-packages\\langsmith\\client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the \n",
       "question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the \n",
       "answer concise.\n",
       "Question: <span style=\"font-weight: bold\">(</span>question goes here<span style=\"font-weight: bold\">)</span> \n",
       "Context: <span style=\"font-weight: bold\">(</span>context goes here<span style=\"font-weight: bold\">)</span> \n",
       "Answer:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the \n",
       "question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the \n",
       "answer concise.\n",
       "Question: \u001b[1m(\u001b[0mquestion goes here\u001b[1m)\u001b[0m \n",
       "Context: \u001b[1m(\u001b[0mcontext goes here\u001b[1m)\u001b[0m \n",
       "Answer:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> The post is about the potential of using large language models <span style=\"font-weight: bold\">(</span>LLM<span style=\"font-weight: bold\">)</span> as the core controller for autonomous agents.\n",
       "It discusses how LLM can be used to generate well-written copies, stories, essays, and programs, and how it can \n",
       "also function as a powerful general problem solver. The post also outlines the key components of a LLM-powered \n",
       "autonomous agent system, including planning, reflection, and memory.\n",
       "</pre>\n"
      ],
      "text/plain": [
       " The post is about the potential of using large language models \u001b[1m(\u001b[0mLLM\u001b[1m)\u001b[0m as the core controller for autonomous agents.\n",
       "It discusses how LLM can be used to generate well-written copies, stories, essays, and programs, and how it can \n",
       "also function as a powerful general problem solver. The post also outlines the key components of a LLM-powered \n",
       "autonomous agent system, including planning, reflection, and memory.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(api_key=openai_key)\n",
    "\n",
    "question = \"what is this post about?\"\n",
    "\n",
    "retrieved_docs = vector_store.similarity_search(question)\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "promptAnswer = prompt.invoke({\"question\": question, \"context\": docs_content})\n",
    "answer = llm.invoke(promptAnswer)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
